For installation of Docker refer to - https://docs.docker.com/engine/install


docker version   --> to get version of installed docker

docker run <image>	-->download image and run a container from an image and exits immidiately, docker ps will not show this container and docker ps -a will show this as exited container because a container only lives as long as the process inside it is alive

docker run -d --name webapp nginx:1.14-alpine  --> give custom name


to run a container even without a process being run inside run use -
docker run ubuntu sleep 5  --> ubuntu does not contain any running process/application by default

appned a command to docker run
docker run ubuntu cat /etc/*release*

docker ps			--> list all running containers details

docker ps -a			--> list all running/stopped containers details

docker stop <container-name or id>	--> container name we can get from using docker ps command

docker rm <container-name or id>	--> to remove a stopped container permanently

docker images	--> to list all the images presented on host

docker rmi <image-name or id>	--> remove image from host(make sure no containers are running from that image, stop and delete all the dependent containers)

docker pull --> to just download image (does not run a container of that image)

to execute a command on running container, example to see content of a file in running ubuntu container

docker exec <container-id or name> cat /etc/hosts

run attach and detach

docker run <container-name>
normal run command runs container in foreground i.e. in attached mode. we will be attached to a console and see the output of application/process on screen. we will not be do anything on this console other than view output untill container stops
press ctrl+c to stop container and application hosted on the container

docker run -d <container-name>   to run a container in detach mode, it will run the docker container in background mode and you will be back to prompt immidiately

to attach later while running use -
docker attach <container-id or name>

=====================================================

docker run redis --> this will pull latest version on redis available

docker run redis:4.0  --> this will pull specific TAG version

by default docker container does not listen to standard input, it does not have a terminal to read input. it runs in a non interactive mode. if you would like to provide your input, you must map standard input of your host to to docker container using "-i" command

docker run -i <application which takes input>

"t" option is to attach container terminal(psuedo terminal)

docker run -it <application which takes input>

port mapping - application running inside a container gets a port which can only be accessible inside docker container. to access through host's web browser we must map a free port of host to application port on which application is running inside container

docker run -p 80:5000 <application-id> --> all traffic coming to port 80 of host will be routed to port 5000 of docker container


once you run a container using plain docker run command and afer your work is done, you remove a container, all the data inside it will be thrown away

if you need to persist data, you need to map a directory outside container on docker host to directory inside a container
ex-  when we run a mysql container, all the data files are stored in /var/lib/mysql directory, using -

docker run -v /opt/datadir:/var/lib/mysql mysql


to get additional details of docker container use-

docker inspect <container-name or id>  --> it return all the details in json format

how to get container logs, launched in background(detached mode)
docker logs <container-name or id>

=========================================================
creating docker image - it might be the case that you cannot find the image of your need at docker hub, the you need to create your own image.
every new image will be created using old image available. Every dockerfile starts with FROM <OS-name>
When Docker builds the images, it builds these in a layered architecture.
Each line of instruction creates a new layer in the Docker image with just the changes from the previous layer.
All the layers/steps built are cached by Docker.It will reuse the previous layers from cache and continue to build the remaining layers.
first create a docker file file by listing all the required step in a specific order(installation of softwares) then use -

docker build . -f dockerfile -t <tag-name for image>
this will create image on your local file system, to make it public on docker hub use -
docker push <tag-name of image created>

============================================================
passing a environment variable to an app -

docker run -e APP_COLOR=blue <app_image>

docker inspect <container-id or name> --> Config section of json will show all the environment variables in a running container
===========================================================

CMD ["bash"]   --> dockerfile must contain CMD at last , means it runs bash command here(in ubuntu image docker file).
bash is shell which listens for input from terminal, if it cannot find a terminal it exits. docker launches an ubuntu container and launch bash program
by default, docker does not attach a terminal to container when it is run. hence bash program exits and "docker ps -a" shows this container in exited state

CMD sleep 5   --> it tells which command to run when container is launched. if you wan to make an image to sleep to 10 seconds instead of 5 then
you need to run -
docker run ubuntu-sleeper sleep 10   --> so the command line parameter passed will get replaced entirely
-------------------------------------
ENTRYPOINT ["sleep"]   --> entrypoint instruction is like a command instruction. it specify the command/program that will be run when container starts
what ever is passed to this image as parameter will be appended to entrypoint command.
i.e.
docker run ubuntu-sleeper 10   --> it will run "sleep 10" when container is launched

if you run "docker run ubuntu-sleeper" to an image having ENTRYPOINT ["sleep"] , then it will throw an error

to over come error, you need to combine both ENTRYPOINT + CMD
i.e.

FROM ubuntu
ENTRYPOINT ["sleep"]
CMD["5"]

If we need to override ENTRYPOINT instruction at runtime then use-
docker run --entrypoint sleep2.0 ubuntu-sleeper 10

=================================================================================================================

Docker compose is used when we are hosting a complex application running multiple services.
with docker compose, we can create a yml file called docker-compose.yml

use "docker-compose up" to bing up entire stack. this is only applicable on running container on single host.

"--link redis:redis" is command line option to link 2 containers together. first redis is name of container and second redis is name of host that app is looking for(came from code)

when we deploy the application with docker-compose.yml file, it will automatically create a network for the containers and
connect all containers to that network.
All containers within application will be able to reach each other using service name given in yml file. hence we dont need
--link section in yml file for docker-compose.yml version 2 and above.
==========================================================================================
docker registry - if the containers where the rain.They will rain from the docker registry which are the clouds.
That's where darker images are stored.
Its a central repository of all Docker images.

image:nginx  --> it internally maps to image:docker.io/nginx/nginx --> docker.io(docker hub) is registry

"docker login privat-registry.io" command is used to login to private registry
==========================================================================================
 Docker engine as we have learned before is simply referred to a host with Docker installed on it.
 When you install Docker on a linux host you're actually installing three different components.
 The Docker demon, The REST API server and the docker CLI.

 The docker demon is a background process that manages Docker objects such as the images containers volumes and networks.
 The Docker rest API server is the API interface that programs can use to talk to the demon and provide
 instructions.
 Docker CLI is nothing but the command line interface that we've been using until now to perform actions such as running a container stopping containers
 destroying images etc. It uses the rest api to interact with the docker demo.

 Something to note here is that the Dockers CLI need not necessarily be on the same host.
 It could be on another system like a laptop and can still work with a remote Docker engine.
 we can use -H option
 docker -H=remote-docker-engine:2375  run nginx

 Docker uses namespace to isolate workspace process ids network inter process communication mounts and Unix time sharing systems are created in their own namespace thereby providing isolation between containers.

There is no restriction as to how much of a resource a container can use and hence a container may end
up utilizing all of the resources on the underlying host.

 Docker uses three groups or control groups to restrict the amount of hardware resources allocated to each container.

 docker run --cpus=.5 ubuntu   --> .5 will ensure that the container does not take up more than 50 percent of the host.

 docker run --memory=100m ubuntu

============================================================================
when you install Docker on a system it creates this folder structure at var/lib/docker you have multiple folders under it called aufs
containers image volumes etc..

Files in image layer are Read only and files in Container layer are Read/Write.

docker volume creat data_volume   --> creates  data_volume dir in /var/lib/docker/volumes
docker run -v data_volume:/var/lib/mysql mysql
we can use mount command as well to map directory on host to container's.

Docker uses storage drivers to enable layered architecture.Creating a writable layer moving files across layers to enable copy and write etc. It's the storage drivers.
Some of the common storage drivers are AUFS, BTRFS, ZFS, device-mapper, overlay and overlay 2.
Docker will choose the best stories driver available automatically based on the operating system like
AUFS is for ubuntu.

docker info  --> it gives details of docker installed on system

You could use the "docker history <image-id>" command to get a list of steps that were followed to create that image.

So if you run the "docker images" command and the size that you see here is actually the total space for each image(including duplicates)
If you wanted to see the actual space consumption on the desk run the "docker system df" or "docker system df -v" command. this shows you the actual disk usage of Docker so if you just run the "docker system df"
command it will show you the disk consumption by images containers and local volumes.(excluding duplicates)

"docker network ls"  -->tells number of networks that exist on system.
When you install Docker it creates three networks automatically bridge, none and host.

Bridge is the default network a container gets attached to.If you would like to associate the container with any other network you specify the network information using the network command line parameter.

docker run ubuntu     --> for bridge, no need to pass anything. its default
docker run ubuntu --network=none
docker run ubuntu --network=host

Bridge network is a private internal network created by Docker on the host.

All containers attached to this network by default and they get an internal IP address usually in the range 172.17 series.

The containers can access each other using this internal IP if required.

To access any of these containers from the outside world map the ports of these containers to ports on the docker host.


Host network- This takes out any network isolation between the docker host and the docker container.
Meaning if you were to run a web server on Port 5000 in a web container it is automatically as accessible
on the same port externally without requiring any port mapping as the web container uses the hosts network.
This would also mean that unlike before you will now not be able to run multiple web containers on the
same host on the same port as the ports are now common to all containers in the host.

none network - The containers are not a test to any network and doesn't have any access to the external network or other containers they run in an isolated network.

User Defined network -
docker network create --driver  bridge --subnet 182.18.0.0/16 custom-isolated-network
"docker network inspect <network-name>" to get all details of a network
use "docker inspect <container name or id>" to get all network related details.

All containers in a docker host can resolve each other with the name of the container Docker has a
built in DNS server that helps the containers to resolve each other using the container name.
Note that the built in DNS server always runs at address 127.0.0.11.

===================================================================================

Container Orchestration consists of a set of tools and scripts that can help host containers in a production environment.

Typically a container orchestration solution consist of multiple Docker hosts that can host containers
that way even if one fails application is still accessible.

"docker service create --replicas=100 nodeApp"

three mostly used container orchestration tools are-
1. Docker Swarm
2. Google Kubernetes
3. Apache Mesos